# 训练可视化指南 - Training Visualization Guide

## 📊 概述

本指南详细解释了训练过程中生成的各类图表及其子图的含义。所有图表已保存在 `fig_plot/` 目录中。

---

## 🎯 图表类型

### 1️⃣ 综合训练曲线 (training_curves_*.png)

这是一个 **2×2 的四子图组合**，展示训练过程的多个关键指标。

#### **左上：策略梯度损失 (Policy Gradient Loss)** - [0,0]

```
数据来源：train/policy_gradient_loss (from TensorBoard)
颜色：#FF6B6B (红色)
图表位置：[0,0] (左上)
```

**含义：**
- 衡量**策略网络学习质量**的关键指标
- 代表策略梯度估计的准确性和稳定性
- 计算公式：`-log(π(a|s)) * A`，其中 A 是优势函数（Advantage）

**为什么会是负值？**
- 这是正常的！PPO 优化目标是最大化策略概率的对数
- `-log(π(a|s))` 项在目标是最大化奖励时自然是负值
- **主要观察是趋势变化**，而不是绝对值

**解读标准：**
| 现象 | 含义 | 处理建议 |
|------|------|---------|
| 平稳下降或稳定在低值 ✓ | 策略在稳定学习 | 保持当前参数 |
| 振荡较大 ⚠️ | 学习率太高，或环境随机性大 | 降低 learning_rate 或增加 n_cpu |
| 持续上升 ❌ | 策略退化或梯度发散 | 检查奖励设计或紧急降低学习率 |
| 快速收敛到-0.01~-0.05之间 ✓ | 策略已收敛 | 正常现象，可以停止训练 |

**代码位置：** `train_highway_ppo.py` 第 227-233 行

---

#### **右上：价值函数损失 (Value Loss)** - [0,1]

```
数据来源：train/value_loss (from TensorBoard)
颜色：#4ECDC4 (青色)
图表位置：[0,1] (右上)
```

**含义：**
- 衡量**价值网络的拟合质量**
- 价值网络用于估计状态的**相对价值**（相对于基线的优势）
- 计算公式：`MSE(V_predict - V_target)` = 平均平方误差
- 是 PPO 算法中计算优势函数的基础

**Value Loss 为什么比 Policy Loss 高？**
- 不同的损失量级
- Policy Loss 权重系数是 1.0
- Value Loss 权重系数是 0.5
- 所以 Value Loss 的绝对值可以更高而仍然是合理的

**解读标准：**
| 现象 | 含义 | 处理建议 |
|------|------|---------|
| 单调递减 ✓ | 价值函数在正常收敛 | 保持训练 |
| 收敛到低值（<5） ✓ | 价值函数已充分学习状态价值 | 正常完成，可继续或停止 |
| 缓慢下降（>300步还很高） ⚠️ | 价值网络容量可能不足 | 增加网络大小（如 [512,256] → [768,512]） |
| 突然上升 ⚠️ | 遭遇分布变化或过拟合 | 考虑降低学习率或增加正则化 |
| 始终很高（>30） ❌ | 值函数网络未能学习 | 检查网络架构、学习率或数据质量 |

**代码位置：** `train_highway_ppo.py` 第 235-241 行

---

#### **左下：总损失 (Total Loss)** - [1,0]

```
数据来源：train/loss (from TensorBoard)
颜色：#95E1D3 (浅绿色)
图表位置：[1,0] (左下)
```

**含义：**
- **综合损失函数**，由以下部分加权组成：
  - `L_policy` - 策略梯度损失
  - `0.5 * L_value` - 价值函数损失（权重0.5）
  - `-0.01 * H(π)` - 熵正则化损失（负号表示最大化）

- **完整公式：** `L_total = L_policy + 0.5 * L_value - 0.01 * H(π)`

**为什么需要总损失？**
- PPO 是一个**多目标优化**问题
- 需要平衡三个目标：
  1. 优化策略性能
  2. 准确估计状态价值
  3. 保持策略多样性（通过熵）

**解读标准：**
| 现象 | 含义 | 处理建议 |
|------|------|---------|
| 整体下降趋势 ✓ | 训练进行良好，三个目标都在优化 | 继续训练 |
| 收敛到较低值（<5） ✓ | 所有损失项都已优化，达到好的平衡点 | 可以停止 |
| 波动剧烈（波幅>20） ⚠️ | 学习不稳定，可能学习率过高 | 降低 learning_rate（如 3e-4 → 1e-4） |
| 在中等水平停滞（>10）| 可能陷入局部最优 | 增加熵系数或增加训练步数 |
| 持续上升或发散 ❌ | 训练发散，优化失败 | 检查奖励函数范围、紧急停止并降低学习率重新开始 |

**代码位置：** `train_highway_ppo.py` 第 243-259 行

---

#### **右下：平均奖励 (Mean Episode Reward)** - [1,1]

```
数据来源：rollout/ep_rew_mean (from TensorBoard)
颜色：#F7DC6F (黄色) + 半透明填充
图表位置：[1,1] (右下)
```

**含义：**
- **最直接的性能指标**，表示模型在环境中的实际表现
- 计算：每轮评估中所有 episode 的平均奖励
- 单位：根据环境定义的得分（通常 0-100，或负值-正值的范围）

**为什么这是最重要的指标？**
- 其他三个指标（损失）是**优化过程的中间目标**
- 奖励是**真正关心的性能指标**
- 奖励上升 = 模型学到有用的行为

**解读标准（根据场景不同）：**

##### 场景1：高速驾驶 (Highway+Merge)
训练步数：300K | 并行环境：6 | 网络：[384,256]

| 阶段 | 训练步数 | 预期奖励 | 含义 |
|------|---------|---------|------|
| 初期 | 0-10K | -5 ~ 0 | 随机探索阶段，频繁碰撞 |
| 早期学习 | 10K-50K | -2 ~ 5 | 开始学到基本策略，碰撞减少 |
| 中期改进 | 50K-150K | 5 ~ 12 | 策略稳定改进中 |
| 收敛期 | 150K-300K | 12 ~ 18 | 已学到稳定驾驶 |
| 最优 | 250K+ | **15+** | 平稳、高效驾驶，加速/减速频率低 |

##### 场景2：环岛通过 (Roundabout)
训练步数：250K | 并行环境：6 | 网络：[256,256]

| 阶段 | 训练步数 | 预期奖励 | 含义 |
|------|---------|---------|------|
| 初期 | 0-20K | -10 ~ -2 | 频繁碰撞，很难通过 |
| 学习期 | 20K-80K | -5 ~ 0 | 偶尔能通过 |
| 改进期 | 80K-200K | 0 ~ 8 | 能稳定通过但有波动 |
| 收敛期 | 200K-250K | **8+** | 可靠通过 |

##### 场景3：倒车入库 (Parking)
训练步数：500K | 并行环境：4 | 网络：[512,512,256]

| 阶段 | 训练步数 | 预期奖励 | 成功率 | 含义 |
|------|---------|---------|--------|------|
| 初期 | 0-50K | -25 ~ -15 | <10% | 大量碰撞，无效操作 |
| 早期学习 | 50K-150K | -15 ~ -5 | 10-30% | 开始找到目标 |
| 中期进展 | 150K-300K | -5 ~ 5 | 30-50% | 逐渐学会靠近目标 |
| 后期精化 | 300K-450K | 5 ~ 15 | 50-65% | 逐渐精确停车 |
| 收敛期 | 450K-500K | **15+** | **60-75%** | 安全、精确停车 |

**曲线形状解读：**
- 🟢 **平稳上升** → 正常学习，继续训练
- 🟡 **波动上升** → 学习波动但整体趋势好，继续训练
- 🟠 **平台期** → 短期没有改进，可能需要调整或已接近最优
- 🔴 **振荡不前** → 可能学习率不适合或探索不足
- 🔴 **下降趋势** → 可能过拟合或分布变化，考虑停止或降低学习率

**代码位置：** `train_highway_ppo.py` 第 261-270 行

---

### 2️⃣ 奖励详细曲线 (reward_curve_*.png)

这是一个**单图**，专注于**奖励的详细分析**，提供更细致的性能洞察。

#### **图表构成**

```
两条线 + 一个填充区域：

1. 原始数据线    [浅色，半透明青色 #4ECDC4]  Alpha=0.3
2. 平滑数据线    [深色，加粗红色 #FF6B6B]   Linewidth=2.5
3. 填充区域      [浅青色 #4ECDC4]            Alpha=0.1
```

**三个元素的作用：**

| 元素 | 颜色 | 透明度 | 含义 |
|------|------|--------|------|
| 原始奖励线 | 浅青 | 30% | 包含完整的采样噪声，显示训练真实波动性 |
| 平滑奖励线 | 红色 | 100% | 应用20点滑动平均平滑，展示长期趋势 |
| 填充区域 | 浅青 | 10% | 表示原始数据的覆盖范围，可视化波动幅度 |

#### **平滑算法说明**

源代码位置：`train_highway_ppo.py` 第 326-347 行

```python
# 自适应平滑窗口大小
if len(values) > 20:
    smoothed = uniform_filter1d(values, size=20)  # 完整数据集：20点滑动平均
elif len(values) > 5:
    smoothed = uniform_filter1d(values, size=5)   # 中等数据集：5点滑动平均
else:
    smoothed = values  # 数据太少（<5个点）：不平滑
```

**平滑参数选择理由：**
- **窗口大小=20**：平均化最近20个数据点
  - 移除短期随机波动
  - 保留训练的长期趋势
  - 平衡平滑度和细节保留
- 自适应选择保证即使数据少也有有意义的平滑

#### **解读指南**

**好的迹象（训练正常进行）：**
- ✅ **平滑线稳定上升** - 模型在学习
- ✅ **原始线围绕平滑线波动，幅度<10%** - 策略稳定且可靠
- ✅ **最后若干周期的平滑线已进入稳定区间** - 已收敛
- ✅ **填充区域宽度逐渐减小** - 策略波动性下降

**警告信号（需要关注）：**
- ⚠️ **平滑线停滞不前（>50K步无改进）** - 无进一步改进，考虑停止
- ⚠️ **原始线剧烈波动（>30% 相对于平均值）** - 策略不稳定，考虑降低学习率
- ⚠️ **平滑线出现下降** - 可能过拟合或遭遇分布变化，考虑停止
- ⚠️ **填充区域宽度持续增大** - 策略波动性在上升，可能学习失控

**失败信号（需要立即停止）：**
- ❌ **平滑线持续下降** - 模型性能恶化
- ❌ **曲线出现 NaN（无数据点）** - 数值错误，检查日志
- ❌ **原始线出现异常峰值（>500% 波动）** - 可能出现异常事件

---

## 📁 文件位置与场景对应

所有生成的图表按场景和运行存储在 **`fig_plot/`** 目录中：

### Highway+Merge 场景 (Scenario 1)
```
fig_plot/training_curves_highway_merge_run_PPO_1.png
fig_plot/reward_curve_highway_merge_run_PPO_1.png
```
- **训练配置：** 300K steps, 60s/episode, [384,256] network
- **期望收敛时间：** 3-4 小时
- **性能目标：** 最终奖励 ≥ 15
- **优化目标：** 平稳驾驶，加速/减速频率低

### Roundabout 场景 (Scenario 2)
```
fig_plot/training_curves_roundabout_run_PPO_1.png
fig_plot/reward_curve_roundabout_run_PPO_1.png
```
- **训练配置：** 250K steps, 50s/episode, [256,256] network
- **期望收敛时间：** 2.5-3.5 小时
- **性能目标：** 最终奖励 ≥ 10
- **优化目标：** 可靠通过环岛，避免碰撞

### Parking 场景 (Scenario 3)
```
fig_plot/training_curves_parking_run_PPO_1.png
fig_plot/reward_curve_parking_run_PPO_1.png
```
- **训练配置：** 500K steps, [512,512,256] network, batch_size=32
- **期望收敛时间：** 5-6 小时
- **性能目标：** 最终奖励 ≥ 20, 成功率 ≥ 60%
- **优化目标：** 精确停车，最小碰撞

---

## 🔧 如何生成这些图表

### 方法1：自动生成（推荐，在训练完成后）

训练脚本在完成训练后会自动生成所有图表到 `fig_plot/` 目录：

```bash
python train_highway_ppo.py --scenario 1
# 训练完成后自动调用 plot_training_curves()
# 图表自动保存到 fig_plot/
```

### 方法2：手动生成（从已有日志）

如果只想重新生成图表而不需要重新训练：

```python
from train_highway_ppo import plot_training_curves

# 从日志目录生成图表
log_dir = './highway_ppo/highway_merge/run_1/PPO_1'
plot_training_curves(log_dir, 'highway_merge', 'fig_plot')
```

### 方法3：使用专用脚本

```bash
# 重新生成所有场景的图表
python regenerate_plots.py
```

---

## 📊 TensorBoard 标量数据源

所有图表数据都来自 TensorBoard 事件日志中的以下标量：

| 图表位置 | TensorBoard 标量名 | 数据类型 | 频率 | 说明 |
|---------|-----------------|---------|------|------|
| [0,0] | `train/policy_gradient_loss` | 浮点数 | 每回合 | 策略梯度估计损失 |
| [0,1] | `train/value_loss` | 浮点数 | 每回合 | 价值函数预测误差 |
| [1,0] | `train/loss` | 浮点数 | 每回合 | 总体优化目标 |
| [1,1] | `rollout/ep_rew_mean` | 浮点数 | 每回合 | 平均单episode奖励 |
| 奖励曲线 | `rollout/ep_rew_mean` | 浮点数 | 每回合 | 同上，显示平滑版本 |

**TensorBoard 日志位置：**
```
highway_ppo/
├── highway_merge/
│   ├── run_1/PPO_1/events.out.tfevents.1768...  # 训练日志1
│   ├── run_2/PPO_1/events.out.tfevents.1768...  # 训练日志2
│   └── run_3/PPO_1/events.out.tfevents.1768...  # 训练日志3
├── parking/
│   ├── run_1/PPO_1/events.out.tfevents.1768...
│   └── run_2/PPO_1/events.out.tfevents.1768...
└── roundabout/
    ├── run_1/PPO_1/events.out.tfevents.1768...
    └── run_2/PPO_1/events.out.tfevents.1768...
```

---

## 🎓 训练质量评估

### 通过图表评估训练是否成功

**绿灯信号 ✅（训练正常）：**
1. ✅ 策略梯度损失 - 稳定下降或波动幅度小（<20%）
2. ✅ 价值函数损失 - 单调递减并收敛到低值
3. ✅ 总损失 - 整体下降趋势，最终值<5
4. ✅ 平均奖励 - 持续上升，最后进入平台期且稳定

**黄灯信号 ⚠️（需要调整）：**
1. ⚠️ 任何损失持续上升（>20K步）
2. ⚠️ 奖励曲线波动超过30%
3. ⚠️ 收敛太慢（超过预期时间2倍）
4. ⚠️ 平滑后的奖励未达到预期最终值的80%

**红灯信号 ❌（需要重新配置）：**
1. ❌ 任何损失发散（趋向无穷或 >100）
2. ❌ 奖励持续下降（>50K步）
3. ❌ 图表显示"无数据"
4. ❌ 任何值出现 NaN 或 Inf

### 快速检查清单

训练完成后，逐项检查：

```
图表检查清单 (training_curves_*.png):
- [ ] Policy Loss: 平稳下降至 -0.01~-0.05
- [ ] Value Loss: 单调下降，收敛到 <5
- [ ] Total Loss: 整体下降趋势，最终 <5
- [ ] Mean Reward: 最终值达到预期目标
- [ ] 无异常值或NaN

图表检查清单 (reward_curve_*.png):
- [ ] 平滑线稳定上升
- [ ] 原始线围绕平滑线波动 <20%
- [ ] 最后100个采样点的波动幅度 <10%
- [ ] 没有异常峰值或掉落
```

---

## 🔍 常见问题排查

### Q1: 为什么 Policy Loss 一直是负值？

**A:** 这是正常的！Policy Loss 通常是负值：
- 计算公式包含 `-log(π(a|s)) * A`
- 负号是优化目标的一部分
- 负值表示策略在优化
- **主要观察是趋势**（下降 = 好），而不是绝对值

### Q2: 为什么 Value Loss 比 Policy Loss 高很多？

**A:** 完全正常，原因是损失权重不同：
- Policy Loss 权重：1.0
- Value Loss 权重：0.5
- 即使绝对值更高，权重相同的贡献

示例：
```
Policy Loss = -0.05  (权重1.0) → 实际贡献 = -0.05
Value Loss = 10      (权重0.5) → 实际贡献 = 5.0
总损失 = -0.05 + 5.0 = 4.95
```

### Q3: 奖励曲线为什么波动这么大？

**A:** 多个可能原因（检查顺序）：
1. **学习率太高** → 降低 `learning_rate` (如 3e-4 → 1e-4)
2. **采样太少** → 增加 `n_steps` 或 `n_cpu`
3. **环境随机性** → 正常现象，观察平滑线的趋势
4. **网络容量不足** → 增加网络大小
5. **批次太小** → 增加 `batch_size`

### Q4: 图表显示"无数据"该怎么办？

**A:** 检查以下几点（按顺序）：
1. TensorBoard 日志是否存在：
   ```bash
   ls -la highway_ppo/highway_merge/run_1/PPO_1/
   # 应该看到 events.out.tfevents.* 文件
   ```
2. 是否安装了 tensorboard：
   ```bash
   pip install tensorboard
   ```
3. 日志目录路径是否正确：
   ```python
   import os
   assert os.path.exists('./highway_ppo/highway_merge/run_1/PPO_1')
   ```
4. 训练是否实际运行（检查日志文件大小）：
   ```bash
   du -h highway_ppo/highway_merge/run_1/PPO_1/
   # 文件应该 >1MB
   ```

---

## 📈 性能基准

基于当前优化参数的预期性能：

| 场景 | 训练时间 | 最终奖励 | 成功率 | 加速/减速频率 | 关键改进 |
|------|---------|---------|--------|-------------|---------|
| Highway | 3-4h | 15-20 | 85-95% | 12-18次/分钟 ↓40% | 稳定性 +50% |
| Roundabout | 2.5-3.5h | 10-15 | 80-90% | - | 安全 +50% |
| Parking | 5-6h | 20-30 | 60-75% | - | 精度 ↓70% |

---

## 🛠️ 高级：自定义图表

### 修改图表样式

编辑 `train_highway_ppo.py` 中的 `plot_training_curves()` 函数：

```python
# 修改颜色（使用十六进制颜色代码）
axes[0, 0].plot(steps, values, linewidth=2, color='#FF0000')  # 红色
axes[0, 1].plot(steps, values, linewidth=2, color='#00FF00')  # 绿色

# 修改标题和标签
axes[0, 0].set_title('自定义标题', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('自定义X轴标签')

# 修改网格样式
axes[0, 0].grid(True, alpha=0.5, linestyle='--', linewidth=0.5)

# 设置坐标轴范围
axes[0, 0].set_xlim(0, max_steps)
axes[0, 0].set_ylim(-1, 1)
```

### 导出不同格式

在 `plot_training_curves()` 函数中修改保存格式：

```python
# 高质量PDF（适合论文）
plt.savefig(plot_path.replace('.png', '.pdf'), dpi=600, bbox_inches='tight', format='pdf')

# SVG矢量格式（可无限放大）
plt.savefig(plot_path.replace('.png', '.svg'), bbox_inches='tight', format='svg')

# 保留两种格式
plt.savefig(plot_path, dpi=300, bbox_inches='tight', format='png')
plt.savefig(plot_path.replace('.png', '.pdf'), dpi=600, bbox_inches='tight', format='pdf')
```

---

## 📚 参考资源

- **Stable Baselines3 文档：** https://stable-baselines3.readthedocs.io/
- **PPO 论文（必读）：** https://arxiv.org/abs/1707.06347 - "Proximal Policy Optimization Algorithms"
- **TensorBoard 指南：** https://www.tensorflow.org/tensorboard/get_started
- **强化学习基础：** https://spinningup.openai.com/

---

**最后更新：** 2026-01-19  
**版本：** v2.1  
**图表保存位置：** `fig_plot/` 目录  
**数据来源：** TensorBoard 事件日志 (highway_ppo/*/run_*/PPO_*/events.out.tfevents.*)

